- question: What is a matrix?
  answer: A matrix A in C^(m x n) is a rectangular array of m rows and n columns.
    1. Arrange real or complex numbers in m x n grid.
    2. Represents linear transformations or systems A x = b.
    3. Use for eigenvalue problems or system solving.
    Operations like inverse or rank computation take O(n^3). Key in numerical solvers.
- question: What is matrix-vector multiplication?
  answer: Matrix-vector multiplication A x = b combines A in C^(m x n) with x in C^n to produce b in C^m.
    1. Compute b_i = sum_{j=1}^n a_ij x_j for i = 1 to m.
    2. Forms linear combination of A’s columns.
    3. For sparse A, use compressed storage.
    Complexity O(m n), efficient for sparse matrices in PDE solvers.
- question: What are orthogonal vectors?
  answer: Vectors x, y in C^n are orthogonal if x^* y = 0.
    1. Compute inner product x^* y = sum_{i=1}^n x_i^* y_i.
    2. If zero, vectors are orthogonal (perpendicular in R^n).
    3. Verify linear independence for basis construction.
    Complexity O(n). Used in QR factorization and orthogonal bases.
- question: What is the Euclidean norm?
  answer: The Euclidean norm ||x||_2 measures length of x in C^n.
    1. Compute ||x||_2 = sqrt(sum_{i=1}^n |x_i|^2).
    2. Represents geometric length, stable for moderate n.
    3. Use in error analysis or optimization.
    Complexity O(n). Numerically stable, avoids overflow with careful summation.
- question: What is an eigenvalue?
  answer: An eigenvalue lambda of A in C^(n x n) satisfies A x = lambda x, x nonzero.
    1. Solve det(A - lambda I) = 0 for lambda.
    2. Find eigenvector x via (A - lambda I) x = 0.
    3. Compute via QR algorithm, O(n^3).
    Eigenvalues reveal matrix behavior in dynamics, stability analysis.
- question: What is a unitary matrix?
  answer: A matrix Q in C^(n x n) is unitary if Q^* Q = I.
    1. Verify Q^* Q = I, where Q^* is conjugate transpose.
    2. Preserves norms: ||Q x||_2 = ||x||_2.
    3. Compute via QR factorization, O(n^3).
    Used in stable algorithms like SVD, prevents error amplification.
- question: What is the rank of a matrix?
  answer: The rank of A in C^(m x n) is the dimension of its column space.
    1. Compute SVD or Gaussian elimination to find independent columns.
    2. Rank <= min(m, n), equals row space dimension.
    3. Determines solvability of A x = b.
    Complexity O(min(m n^2, m^2 n)). Key in system analysis.
- question: What is a sparse matrix?
  answer: A sparse matrix A in C^(m x n) has mostly zero entries (<10% nonzero).
    1. Store nonzeros in compressed sparse row format.
    2. Use iterative solvers like GMRES, O(nnz) per iteration.
    3. Apply in PDEs or graph algorithms.
    Reduces memory and computation, efficient for large systems.
- question: What is Gaussian elimination?
  answer: Gaussian elimination solves A x = b, A in C^(n x n).
    1. Transform A to upper triangular U via row operations.
    2. Use partial pivoting for stability.
    3. Solve U x = c by back substitution.
    4. Verify solution A x = b.
    Complexity O(n^3), stable with pivoting, used in dense solvers.
- question: What is a positive definite matrix?
  answer: A Hermitian A in C^(n x n) is positive definite if x^* A x > 0 for nonzero x.
    1. Verify x^* A x > 0 or positive eigenvalues.
    2. Compute Cholesky A = R^* R, O(n^3 / 3).
    3. Ensures invertibility, convex optimization.
    Stable, common in energy-based problems.
- question: How is QR factorization computed using Householder reflections?
  answer: QR factorization of A in C^(m x n), m >= n, gives A = Q R.
    1. Initialize Q = I_m, R = A.
    2. For k = 1 to min(m-1, n):
       - Set x = R(k:m, k), v_k = x + sign(x_k) ||x||_2 e_1.
       - Compute H_k = I - 2 v_k v_k^* / (v_k^* v_k).
       - Update R(k:m, k:n) = H_k R(k:m, k:n), Q = Q H_k.
    3. Verify A = Q R, Q^* Q = I.
    Stable, O(m n^2), used in least squares.
- question: How does the Gram-Schmidt process orthogonalize vectors?
  answer: Gram-Schmidt orthogonalizes {a_1, ..., a_n} in C^m to A = Q R.
    1. Initialize Q = [], R = 0_(n x n).
    2. For k = 1 to n:
       - Set u_k = a_k, r_jk = q_j^* a_k for j < k, u_k = u_k - r_jk q_j.
       - Set r_kk = ||u_k||_2, q_k = u_k / r_kk, append q_k to Q.
    3. Verify A = Q R, Q^* Q = I_n.
    Modified version improves stability, O(m n^2).
- question: How is a least squares problem solved using QR factorization?
  answer: Solve min ||A x - b||_2, A in C^(m x n), m > n.
    1. Compute QR: A = Q R, Q orthogonal, R upper triangular.
    2. Compute c = Q^* b.
    3. Extract R_1 (n x n), c_1 (first n entries).
    4. Solve R_1 x = c_1 by back substitution.
    5. Verify residual ||A x - b||_2.
    Stable, O(m n^2), unique for full rank A.
- question: How does power iteration find the dominant eigenvalue?
  answer: Power iteration finds lambda_1 of A in C^(n x n), |lambda_1| > |lambda_2|.
    1. Initialize random x_0, x_0 = x_0 / ||x_0||_2.
    2. For k = 1, 2, ...:
       - Compute y_k = A x_(k-1).
       - Set x_k = y_k / ||y_k||_2, rho_k = x_k^* A x_k.
    3. Stop when |rho_k - rho_(k-1)| < epsilon.
    4. Output rho_k, x_k.
    Linear convergence, O(n^2) per iteration.
- question: How is the condition number of a matrix computed?
  answer: Compute kappa(A) for nonsingular A in C^(n x n).
    1. Compute SVD A = U Sigma V^*, Sigma = diag(sigma_1, ..., sigma_n).
    2. Set sigma_max = sigma_1, sigma_min = sigma_n.
    3. Compute kappa(A) = sigma_max / sigma_min.
    4. If sigma_n = 0, kappa = infinity.
    Complexity O(n^3), measures sensitivity in A x = b.
- question: How does Cholesky factorization solve a linear system?
  answer: Solve A x = b, A Hermitian positive definite in C^(n x n).
    1. Compute Cholesky A = R^* R, R upper triangular.
    2. Solve R^* y = b by forward substitution.
    3. Solve R x = y by back substitution.
    4. Verify A x = b.
    Stable, O(n^3 / 3), efficient for symmetric systems.
- question: How is the singular value decomposition (SVD) computed?
  answer: Compute SVD of A in C^(m x n): A = U Sigma V^*.
    1. Form B = A^* A, compute eigenvalues lambda_i.
    2. Set sigma_i = sqrt(lambda_i), sort decreasing.
    3. Form V from B’s eigenvectors, U from A v_i / sigma_i.
    4. Extend U for null(A^*).
    5. Verify A = U Sigma V^*.
    Complexity O(min(m n^2, m^2 n)), used for rank, norms.
- question: How does GMRES solve a linear system?
  answer: GMRES solves A x = b, A in C^(n x n).
    1. Initialize x_0, r_0 = b - A x_0, v_1 = r_0 / ||r_0||_2.
    2. For k = 1 to m:
       - Compute w = A v_k, orthogonalize w, set h_jk, v_(k+1).
       - Form Hessenberg H_k, solve min ||beta e_1 - H_k y||_2.
    3. Update x_k = x_0 + V_k y.
    4. Stop when ||r_k||_2 < epsilon.
    O(n^2 m), stable, converges in <= n steps.
- question: How is LU factorization with partial pivoting performed?
  answer: Compute P A = L U for A in C^(n x n).
    1. Initialize P = I_n, L = I_n, U = A.
    2. For k = 1 to n-1:
       - Pivot: swap row k with max |u_ik|, update P.
       - Compute l_ik = u_ik / u_kk, update L, U.
    3. Output P, L, U.
    4. Verify P A = L U.
    Stable, O(n^3), solves A x = b.
- question: How does the QR algorithm compute eigenvalues?
  answer: Compute eigenvalues of A in C^(n x n).
    1. Reduce A to Hessenberg H via Householder.
    2. Initialize A_0 = H.
    3. For k = 1, 2, ...:
       - Choose shift mu_k (e.g., Wilkinson).
       - Factor A_k - mu_k I = Q_k R_k.
       - Set A_(k+1) = R_k Q_k + mu_k I.
    4. Stop when subdiagonal < epsilon.
    O(n^3), cubic convergence for symmetric A.
- question: How is a matrix reduced to Hessenberg form?
  answer: Reduce A in C^(n x n) to Hessenberg H.
    1. For k = 1 to n-2:
       - Set x = A(k+1:n, k), v_k = x + sign(x_1) ||x||_2 e_1.
       - Compute H_k = I - 2 v_k v_k^* / (v_k^* v_k).
       - Update A(k+1:n, k:n), A(1:n, k+1:n).
    2. Output H = A.
    3. Verify H = Q^* A Q.
    O(n^3), accelerates QR algorithm.
- question: How does the conjugate gradient method solve Ax = b?
  answer: Solve A x = b, A Hermitian positive definite in C^(n x n).
    1. Initialize x_0, r_0 = b - A x_0, p_0 = r_0.
    2. For k = 0, 1, ...:
       - Compute alpha_k = (r_k^* r_k) / (p_k^* A p_k).
       - Update x_(k+1), r_(k+1), beta_k, p_(k+1).
    3. Stop when ||r_(k+1)||_2 < epsilon.
    O(n^2) per iteration, converges in <= n steps, stable.
- question: How is the pseudoinverse of a matrix computed?
  answer: Compute A^+ for A in C^(m x n).
    1. Compute SVD A = U Sigma V^*, Sigma = diag(sigma_1, ..., sigma_r, 0, ...).
    2. Form Sigma^+ with 1/sigma_i for sigma_i > 0, transpose.
    3. Compute A^+ = V Sigma^+ U^*.
    4. Verify A A^+ A = A.
    O(min(m n^2, m^2 n)), solves least squares, minimum-norm solutions.
- question: How does inverse iteration find an eigenvector?
  answer: Find eigenvector of A in C^(n x n) for eigenvalue near mu.
    1. Initialize random x_0, x_0 = x_0 / ||x_0||_2.
    2. For k = 1, 2, ...:
       - Solve (A - mu I) y_k = x_(k-1).
       - Set x_k = y_k / ||y_k||_2.
    3. Stop when ||x_k - x_(k-1)||_2 < epsilon.
    O(n^3) per iteration, linear convergence, faster with good mu.
- question: How is a low-rank approximation constructed using SVD?
  answer: Construct rank-k approximation of A in C^(m x n).
    1. Compute SVD A = U Sigma V^*, Sigma = diag(sigma_1, ..., sigma_r, 0, ...).
    2. Choose k < r.
    3. Form Sigma_k = diag(sigma_1, ..., sigma_k, 0, ...).
    4. Compute A_k = U Sigma_k V^*.
    5. Verify ||A - A_k||_2 = sigma_(k+1).
    O(min(m n^2, m^2 n)), optimal for compression.
- question: How does the Arnoldi iteration approximate eigenvalues?
  answer: Approximate eigenvalues of A in C^(n x n).
    1. Initialize v_1 = b / ||b||_2, V_1 = [v_1].
    2. For k = 1 to m:
       - Compute w = A v_k, orthogonalize w, set h_jk, v_(k+1).
       - Update V_(k+1), H_k.
    3. Compute eigenvalues of H_m (Ritz values).
    O(n^2 m), extreme eigenvalues converge first, used for sparse A.
- question: How is a Vandermonde matrix solved efficiently?
  answer: Solve V x = b, V in C^(n x n), v_ij = x_i^(j-1).
    1. Recognize V x = b as polynomial interpolation.
    2. Compute Lagrange basis L_i(t) = prod_{j != i} (t - x_j) / (x_i - x_j).
    3. Solve x_i = b_i / prod_{j != i} (x_i - x_j).
    4. Use FFT, O(n log^2 n).
    High kappa requires high-precision arithmetic.
- question: How does the Lanczos iteration compute eigenvalues?
  answer: Approximate eigenvalues of Hermitian A in C^(n x n).
    1. Initialize v_1 = b / ||b||_2, v_0 = 0, beta_0 = 0.
    2. For k = 1 to m:
       - Compute w = A v_k - beta_(k-1) v_(k-1).
       - Set alpha_k, beta_k, v_(k+1).
    3. Form tridiagonal T_m, compute its eigenvalues.
    O(n m) for sparse A, extreme eigenvalues converge fastest.
- question: How is preconditioning applied in iterative methods?
  answer: Improve iterative methods for A x = b, A in C^(n x n).
    1. Choose M ≈ A (e.g., incomplete LU).
    2. Rewrite M^(-1) A x = M^(-1) b.
    3. Apply GMRES or CG, solving M z = r each iteration.
    4. Stop when ||b - A x_k||_2 < epsilon.
    O(n^2) per iteration, good M reduces kappa(M^(-1) A).
- question: How does Rayleigh quotient iteration refine eigenvalues?
  answer: Refine eigenvalue of A in C^(n x n).
    1. Initialize x_0, rho_0 = x_0^* A x_0.
    2. For k = 1, 2, ...:
       - Solve (A - rho_(k-1) I) y_k = x_(k-1).
       - Set x_k = y_k / ||y_k||_2, rho_k = x_k^* A x_k.
    3. Stop when |rho_k - rho_(k-1)| < epsilon.
    Cubic convergence, O(n^3) per iteration.
- question: What does the Fundamental Theorem of Linear Algebra state?
  answer: For A in C^(m x n):
    1. Compute rank(A) via SVD, equals dim(range(A)) = dim(range(A^*)).
    2. Solve A x = 0 for null(A), dim = n - rank(A).
    3. Solve A^* y = 0 for null(A^*), dim = m - rank(A).
    4. Verify range(A) ⊥ null(A^*), range(A^*) ⊥ null(A).
    O(min(m n^2, m^2 n)), decomposes C^m, C^n, key for SVD, systems.
- question: What is the Eckart-Young-Mirsky Theorem?
  answer: For A in C^(m x n), best rank-k approximation A_k:
    1. Compute SVD A = U Sigma V^*.
    2. Form Sigma_k with top k singular values.
    3. Set A_k = U Sigma_k V^*.
    4. Verify ||A - A_k||_2 = sigma_(k+1), ||A - A_k||_F = sqrt(sum_{i=k+1}^r sigma_i^2).
    O(min(m n^2, m^2 n)), used in compression, noise reduction.
- question: What does the Spectral Theorem for Hermitian matrices state?
  answer: For Hermitian A in C^(n x n):
    1. Compute eigenvalues via QR algorithm, all real.
    2. Form Q from orthonormal eigenvectors, Lambda from eigenvalues.
    3. Verify A = Q Lambda Q^*, Q^* Q = I.
    4. Note orthogonal eigenvectors for distinct eigenvalues.
    O(n^3), enables stable spectral decompositions in optimization.
- question: What is the Schur Factorization Theorem?
  answer: For A in C^(n x n):
    1. Reduce A to Hessenberg via Householder.
    2. Apply QR algorithm: A_k - mu_k I = Q_k R_k, A_(k+1) = R_k Q_k + mu_k I.
    3. Converge to T, U from Q_k products.
    4. Verify A = U T U^*, T’s diagonal has eigenvalues.
    O(n^3), stable, generalizes eigenvalue decomposition.
- question: What does the QR Decomposition Theorem guarantee?
  answer: For A in C^(m x n), m >= n:
    1. Compute QR via Householder: A = Q R.
    2. Verify Q^* Q = I, R upper triangular.
    3. For full rank A, check R(1:n, 1:n) nonsingular.
    4. Ensure R’s diagonal positive for uniqueness.
    O(m n^2), stable for least squares, eigenvalue problems.
- question: What is the LU Factorization Theorem?
  answer: For A in C^(n x n):
    1. If leading minors nonzero, compute A = L U via Gaussian elimination.
    2. For general A, use partial pivoting: P A = L U.
    3. Verify P A = L U, L lower triangular, U upper triangular.
    4. Solve A x = b using L, U.
    O(n^3), stable with pivoting.
- question: What does the Cholesky Factorization Theorem state?
  answer: For Hermitian positive definite A in C^(n x n):
    1. Compute A = R^* R, R upper triangular, positive diagonal.
    2. For j = 1 to n:
       - Set r_jj = sqrt(a_jj - sum_{k=1}^{j-1} |r_kj|^2).
       - Set r_ij for i > j.
    3. Verify A = R^* R.
    O(n^3 / 3), stable, efficient for symmetric systems.
- question: What is the Singular Value Decomposition Theorem?
  answer: For A in C^(m x n):
    1. Form B = A^* A, compute eigenvalues lambda_i.
    2. Set sigma_i = sqrt(lambda_i), form Sigma.
    3. Build U, V from eigenvectors, null(A^*).
    4. Verify A = U Sigma V^*, U, V unitary.
    O(min(m n^2, m^2 n)), reveals rank, norms, approximations.
- question: What does the Jordan Canonical Form Theorem state?
  answer: For A in C^(n x n):
    1. Find eigenvalues via det(A - lambda I) = 0.
    2. Solve (A - lambda_i I)^k x = 0 for generalized eigenvectors.
    3. Form P, J with Jordan blocks J_i = lambda_i I + N.
    4. Verify A = P J P^(-1).
    Numerically unstable, used theoretically, not in practice.
- question: What is the Perron-Frobenius Theorem?
  answer: For A in R^(n x n) with positive entries:
    1. Compute lambda_max via power iteration, lambda_max > 0.
    2. Verify lambda_max simple, positive eigenvector.
    3. Check |lambda| <= lambda_max for other eigenvalues.
    4. Apply in Markov chains, network analysis.
    O(n^2) per iteration, governs asymptotic behavior.